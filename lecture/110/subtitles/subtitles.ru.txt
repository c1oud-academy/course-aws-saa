Добрый день, уважаемые студенты! Я рад вас всех видеть на очередной лекции. Мы с вами начинаем серию лекций, посвящённые Networking. На этой неделе мы поговорим про настройку сети в облаке AWS. На второй неделе у нас не будет лекций, но будет объёмная лабораторная работа. Она будет включать все те знания, которые вы получили на предыдущих лекциях, а также материалы, пройденные на последней лекции. На третьей неделе мы рассмотрим сервисы и варианты подключения локальной сети к сети в облаке AWS. Итак, давайте начнём. Сегодняшняя лекция состоит из четырёх частей. В первой части мы рассмотрим следующие улучшения нашей архитектуры в облаке, которые мы будем проделывать в рамках лабораторных работ. Во второй части подробнее познакомимся с сервисом VPC и его возможностями. В третьей части мы рассмотрим нюансы, связанные с VPC и его компонентами, которые дают нам возможность настроить подключение к интернету. В последней части, четвёртой, мы рассмотрим нюансы, связанные с обеспечением безопасности нашего VPC. В первой части мы обсудим изменение инфраструктуры в облаке AWS в рамках лабораторных работ. Вы видите, что у нас появились новые компоненты, это VPC, Internet gateway, NAT gateway и три subnets. Это всё работает согласованно и, во-первых, даёт большую безопасность, а во-вторых, у нас также все те ресурсы, к которым нужен выход в интернет, они также остались с этим доступом. Если говорить про бизнес-кейс, то работа в кафе продвигается хорошо, клиентов становится больше. София и Нихил освободились от своих основных работ и начали обсуждение по дальнейшему развитию IT-инфраструктуры в облаке AWS. Они посоветовались с постоянными посетителями кафе, которые работают с AWS, в нашем случае это Оливия, она является Solutions Architect и, выслушав текущее состояние инфраструктуры наших главных героев, она подсказала, что следует обратить особое внимание правильной настройке VPC, то есть сети и всех ее компонентов, чтобы обезопасить инфраструктуру. Также она посоветовала для удаленного подключения и возможности проводить некоторые работы настроить Bastion Host. Что такое Bastion Host и другие компоненты мы с вами узнаем в нашей сегодняшней лекции. И мы сейчас переходим ко второй части нашей сегодняшней лекции. Во второй части сегодняшней лекции мы подробнее познакомимся с сервисом VPC и её компонентами. Что такое VPC? VPC расшифровывается как Virtual Private Cloud и оно нам предоставляет логически изолированную часть в облаке AWS. Оно максимально приближено к локальной сети, которую мы с вами настраивали много раз дома, на работе и AWS дает возможность удаленно настроить это в облаке AWS. У нас есть возможность выбирать CIDR блоки, то есть какой диапазон IP адресов мы хотим использовать для нашей локальной сети. Мы также можем создавать любое количество subnets в рамках размера нашего VPC. Внутри VPC мы можем настроить необходимые правила, чтобы трафик мог перемещаться в таком или ином виде. Есть также все необходимые компоненты для обеспечения безопасности на уровне subnet либо на уровне отдельно взятых ресурсов. Отличие при работе с AWS от локальных работ это то, что вам нет необходимости работать с железом. То есть вы не будете подключать кабель к сети, как-то их готовить и так далее. Все эти работы они уже проведены на стороне дата-центра AWS, уже все готово к использованию и вы удаленно, достаточно быстро можете настроить локальную сеть в облаке в зависимости от вашей бизнес задачи. VPC создается в рамках определенного AWS региона. То есть вы не можете создать VPC, который будет работать в нескольких AWS регионах. Если же вам нужно cross-region взаимодействия, то в каждом регионе создается отдельная VPC и эти VPC вы соединяете между собой. Как это делать мы с вами пройдем на следующих наших занятиях. Если говорить про availability зоны, то для VPC доступны все availability зоны в рамках региона, на котором оно было создано. Вы указываете все либо некоторые из availability зон, с которыми вы будете работать и соответственно ресурсы в рамках этого VPC могут быть созданы в той или иной availability зоне. Зачастую во время создания VPC указывают как минимум две availability зоны, что дает возможность построить highly available инфраструктуру в облаке AWS. Таким образом вы заранее обезопасите себя от возможных проблем в рамках availability зоны. В случае если что-то произойдет с одной availability зоной, другая часть вашей инфраструктуры продолжит работать во второй availability зоне. Во время создания VPC обязательно необходимо указать CIDR блок. CIDR блок - это диапазон IP адресов доступных для ресурсов создаваемых внутри VPC. Расшифруется как Classless Inter-domain routing. CIDR блок обозначается следующим форматом. Указывается начальный IP адрес и через слеш указывается размер CIDR блока. Размер CIDR блока может быть от 0 до 32. 0 - это самый большой теоретически возможный CIDR блок, а 32 - это самый маленький, который состоит из одного IP адреса. Если вы посмотрите примеры, первая строчка это все нули и IP адрес, а также размерность CIDR блока 0. Оно является фактически всем интернетом и затрагивает все теоретически возможные IPv4 адреса. Если мы говорим про CIDR блок размера 32, то здесь у нас один единственный IP адрес и тот адрес, который начальный и является адресом внутри этого CIDR блока. В нашем случае - это 10.22.33.44. Давайте рассмотрим другой пример. Достаточно частый кейс, это CIDR блоки размера 24. Для этого CIDR блока доступны 256 последовательных IPv4 адресов. Если мы говорим, что у нас начальный IP адрес 10.22.33.0, то есть вы видите последний компонент IP адреса 0, то самым последним является 255, то есть 10.22.33.255. Таким образом в этом CIDR блоке ровно 256 IP адресов с учетом того, что IP адрес начинается с нуля. Чтобы проще это обозначать, вместо последнего четвертого компонента указываем * звездочку. Таким образом 10.22.33.* говорит нам о том, что в этом CIDR блоке 256 IP адресов. Давайте рассмотрим еще один популярный пример. Это CIDR блок размера 16 и в этом случае он содержит 65536 IP адресов. Это высчитывается следующим образом. Нам необходимо от 32 отнять размер нашего CIDR блока и возвести в эту степень двойки. В нашем случае это будет 2 в степени 16. Оно в себе содержит 65000 адресов и для простоты обозначается следующим образом. Первые два компонента указывается и оставшиеся два, третий и четвертый указывается как звездочка. Таким образом мы понимаем, что каждый из третьего и четвертого компонента IP адреса может принимать значение от нуля до 255. Если мы говорим касательно AWS, то минимальный размер CIDR блока в рамках AWS - это 28. 28 - это 16 IP адресов. Следует обратить внимание, что AWS резервирует для системного использования 5 IP адресов. Таким образом в CIDR блоке 28 у вас теоретически есть 16 IP адресов, а в AWS вы отнимаете еще 5 адресов и для вас будут доступны 11 IP адресов. Самый большой максимально возможный CIDR блок в рамках AWS - это 16. Оно содержит в себе 65536 IP адресов. Это число запоминать не нужно. И если говорить фактическое количество IP адресов доступных для вас в AWS, то вы отнимаете также 5 IP адресов, будет 65531. Вам нет необходимости запоминать количество IP адресов в том или ином CIDR блоке. Достаточно знать формулу. Повторюсь, мы это проходили в предыдущих лекциях. Идея в том, что вы берете размер, пусть это будет 24, используйте следующую формулу: 32-24 равняется 8 и вы возводите полученное значение в эту степень двойку. Два в степени 8. Оно равняется 256. При необходимости, вы зная формулу можете рассчитать какое количество IP адресов доступно в любом из CIDR блоков. Я вам рекомендую запомнить лишь некоторые самые популярные CIDR блоки. Это как обозначается весь интернет - все нули, как обозначается один единственный IP адрес - это слеш 32 и несколько CIDR блоков это размера 28, что он состоит из 16 IP адресов. Потом CIDR блок размера 24, что он состоит из 256 IP адресов и самый большой возможный - это /16, который состоит из 65 тысяч IP адресов. Мы с вами помним, что VPC является некоторым изолированным периметром внутри облака AWS. Subnets же они входят в определенный VPC и их следует больше воспринимать как контейнеры или логические группы. Между этими группами вы можете настраивать routing policy. Routing policy мы с вами пройдем в следующей части нашей сегодняшней лекции. Когда вы создаете subnets вам также обязательно как и для VPC необходимо указать CIDR блок. CIDR блоки subnet-ов не должны пересекаться между собой. Это также верно для VPC. CIDR блоки ваших VPC не должны пересекаться между собой в рамках AWS аккаунта. VPC работает поверх нескольких availability zone, но в рамках одного региона. А subnets работают в определенной availability zone и привязываются к нему во время создания. В рамках availability zone можете создать необходимое количество subnets, нет ограничений. Единственное ограничение - это учитывать размеры вашего VPC, а также размеры ваших subnets. На примере справа вы видите, что у нас есть VPC с следующим CIDR блоком. CIDR блок размера 22 - это 1024 IP адресов. Далее, у нас есть 4 subnet, 2 public subnet, 2 private subnet. По одному public subnet и по одному private subnet расположились в каждой availability zone и у каждого subnet есть свой CIDR блок размера 24. 24 - он состоит из 256 IP адресов. А мы с вами помним, что AWS резервирует для себя 5 IP адресов, поэтому для нас доступны 251 IP адрес. Давайте остановимся на основных best practices, связанных с настройкой VPC. Рекомендуется в самом начале для VPC подключить как минимум две или более availability zone и строить ваше приложение таким образом, чтобы оно работало в нескольких availability zone. Таким образом сразу в начале ваше приложение будет высоко доступным. Если же вы начнете с одной availability zone, то в дальнейшем как только вы решите, что ваше приложение должно работать в нескольких availability zone, уже у вас будет какое-то готовое приложение, будут готовые нагрузки и это будет дольше по времени и трудозатратнее изменять вашу существующую архитектуру. Следствием этого best practices является то, что рекомендуется в различных availability zone создавать subnets одинаковых размеров. Это что-то логичное, но бывает такое, что создается в основной availability zone большие subnets большего размера, а в других поменьше. Если это будет проблемой в будущем, если же возникнут проблемы с availability zone, то у вас в других availability zone будут маленькие subnets и есть вероятность того, что не хватит IP адресов для обслуживания вашей нагрузки. Если же это будет равномерно, то вы не будете привязаны к одной конкретной availability zone и можете перекидывать основные нагрузки на любую из существующих availability zone. Далее, следующий best practice - в рамках CIDR блока VPC рекомендуется оставить некоторый запас. То есть, знайте, что VPC CIDR блок и subnet CIDR блок, он настраивается в самом начале, является обязательным параметром и далее он не может быть изменен. В случае subnet не так критично, потому что вы можете этот subnet удалить и создать новый большего размера, а с VPC все намного сложнее, потому что вам в этом случае придется всю вашу инфраструктуру от одного VPC, который поменьше, переносить на VPC новую, которая побольше. Это прям большая проблема, поэтому в самом начале рекомендуется большой запас взять для размера VPC. Тем более мы за размеры VPC не оплачиваем, поэтому есть такая возможность брать большой запас. На вопрос, какой именно запас рекомендуется брать, рекомендую отталкиваться от следующих показателей. Если вы начинающий стартап, неизвестно какие нагрузки будут, вы можете попробовать теоретически рассчитать нагрузку на первые 100, либо 1000, либо 10 тысяч пользователей. Далее, уже примерный расчет этих нагрузок даст понять, какой объем ресурсов вам нужен. Теперь представьте, что рассчитанный эталонный объем нагрузок увеличится в 1000 раз, что произойдет с вашей инфраструктурой, каких размеров она должна быть, чтобы обслужить эти нагрузки. Исходя из этого можете прикинуть оптимальный размер VPC, чтобы в будущем не пришлось его пересоздавать. Если же у вас уже есть нагрузки либо в другом AWS аккаунте, может быть старый VPC, или вы с локальной инфраструктуры мигрируете в облако, то в этом случае у вас скорее всего уже есть определенные нагрузки, они стабильные, и ожидается, что по мере роста компании ваши нагрузки также будут расти. В этом случае нет необходимости перемножать на 1000, это достаточно большой запас. В этом случае достаточно представить нагрузки в 100 раз больше, и в этом случае вы поймете, в каких местах какие ресурсы потребуют масштабирования. А какая-то часть ресурсов, она может остаться такой же и сильно масштабироваться не будет. Исходя из этого вы поймете, какой размер инфраструктуры будет, какое количество ресурсов и соответственно IP адресов вам будет нужно. И исходя из этого вы подберете оптимальный размер CIDR блока вашего VPC. И последний best practice, он также является логичным, но не всегда учитывается компаниями. Часто бывает такое, что компания начинает мигрировать в облаке, у нее есть один AWS аккаунт, далее создается другой AWS аккаунт и другой департамент также начинает работать в другом AWS аккаунте со своим VPC, со своей инфраструктурой. Естественно в какой-то момент возникает вопрос, можно ли это все объединить. Так вот в случае, если у вас CIDR блоки VPC из разных аккаунтов пересекаются, то вы никак не сможете между собой соединить, это практически будет нереально. Поэтому необходимо, чтобы IT архитектор в компании либо технический директор всегда учитывал эти моменты, проходило это все через него централизовано и диапазоны IP адресов, то есть CIDR блоки для VPC, выдавались так, чтобы не было пересечений. В этом случае в будущем, даже если у вас будут отдельные AWS аккаунты и потом вы решите все объединить, у вас не будет проблем со соединением этих VPC между собой. В случае же, если такая ситуация произошла, то единственное решение это ту VPC, которую проще перенести на другой VPC с CIDR блоком не пересекающимся с существующими VPC, мигрировать. Это может потребовать много времени и трудозатрат от команды, поэтому у вас есть возможность, зная эти нюансы, в самом начале двигаться в правильном направлении и избежать возможных проблем в будущем. Давайте теперь подробнее поговорим про VPC deployment, то есть каким образом мы можем настроить VPC в зависимости от нашего приложения, размера компании, команды и так далее. Самый старый и традиционный подход - это использовать один AWS аккаунт и все наши нагрузки, всю нашу инфраструктуру хостить в одном VPC. Это не рекомендуемый подход, но для маленьких, небольших команд и компании, оно может подойти. Более продвинутый вариант - Multiple VPCs, в одном AWS аккаунте создаются отдельные VPC, таким образом вы нагрузки изолируете друг от друга, но тем не менее эти нагрузки находятся в одном AWS аккаунте. Этот подход также подходит для небольших команд и дает возможность кратно масштабироваться в будущем, то есть никак не будет ограничивать команду. И самый продвинутый и рекомендуемый вариант - это Multiple accounts, когда у вас каждый environment вашего приложения диплоится в одном VPC в отдельном AWS аккаунте. Это подходит для любого размера компании, но особо рекомендуется для крупных организаций, компаний, где есть несколько команд, департаментов и каждая команда двигается независимо друг от друга. Тем не менее они все будут объединены между собой и могут управляться централизованно. На текущий момент есть очень много кастомных решений, инструментов, а также нативных сервисов, которые помогают управлять большое количество AWS аккаунтов. Самый яркий пример - это AWS Organizations и AWS Control Tower. Это те сервисы, которые активно развиваются и очень упрощают работу с несколькими AWS аккаунтами. Давайте приведу пример, что может находиться в разных AWS аккаунтах. Представьте у вас B2C приложение, ее используют пользователи, физические лица и в этом случае в каждом AWS аккаунте может находиться отдельный environment. Например, представим в первом у нас Dev среда, во втором у нас среда тестирования, в третьем у нас Pre-Prod и в самом последнем у нас Prod среда, где вы обслуживаете боевые нагрузки. Другой пример, если вы B2B компания, то в этом случае вашими партнерами являются другие компании. Для этих компаний вы можете создавать, выделять несколько AWS аккаунтов, в каждой из которых будет та или иная среда. Например, у вас есть партнер компания A, крупный партнер, для него вы выделили 3 AWS аккаунта для каждого environment. Например, для компании A для тестовой среды один аккаунт, для второго аккаунта мы загрузим среду разработки и в третьей у нас будет находиться Prod среда для этой компании. В этом случае нагрузки на среде разработки никак не отразятся на нагрузке в Production среде. Таким образом, вы максимально изолируете ваши нагрузки друг от друга и это может также быть полезным в рамках некоторых регуляторных требований, которые запрещают или рекомендуют нагрузки от разных компаний либо в зависимости от среды отделять внутри облака. Мы с вами знаем, что у каждого сервиса есть различные лимиты. Лимиты бывают soft и hard. Soft - это те, которые могут быть увеличены, hard - это те, которые не могут быть увеличены. Нужно всегда помнить о hard лимитах, но есть яркий пример лимита в рамках Amazon VPC. Он часто приходит на экзамене, поэтому рекомендуется его запомнить. Для работы оно также вам поможет, но вы всегда можете открыть соответствующий сервис, посмотреть какие лимиты есть у любого из сервисов и дальше принимать решение во время вашей работы. Какой это лимит? В рамках одного AWS аккаунта для каждого региона вы можете создавать не более 5 VPC. Этот лимит является soft, поэтому если вы сделаете запрос, то вам этот лимит могут увеличить. Отлично, мы с вами добрались до конца второй части нашей сегодняшней лекции и подробнее проговорили про сервис VPC, остановились на том, что такое CIDR блок, рассмотрели компонент VPC, subnets и теперь двигаемся дальше к третьей части нашей сегодняшней лекции. Мы с вами начинаем третью часть нашей сегодняшней лекции и здесь посмотрим нюансы, связанные с подключением интернета к нашим ресурсам внутри VPC. Итак, мы создаем VPC, внутри VPC создаются subnets и есть два вида public subnet и private subnet. Здесь очень важный момент, при создании subnet мы нигде не ставим галочку, нет такого переключателя, который делает subnet public или private. Мы лишь в названии subnet-а указываем он public или private, подразумевая, что в случае указания public subnet, мы будем проводить дополнительные настройки, чтобы фактически сделать его public. Что такое public subnet, это когда у ресурсов в этом subnet-е есть выход в интернет и обратное тоже верно, из интернета до ресурсов в public subnet тоже можно добраться, то есть обратиться напрямую. Когда мы говорим private subnet, то там обратный случай, ресурсы находящиеся в private subnet не могут напрямую выйти в интернет и также из интернета к этим ресурсам добраться также невозможно. Давайте теперь рассмотрим основные шаги, которые позволяют наш subnet сделать фактически public. Для этого необходимо использовать компонент Network-а, называется Internet gateway. Internet gateway создается как отдельный ресурс и привязывается к VPC. Создание Internet gateway - это самый первый шаг. Этот компонент является managed, поэтому вам достаточно ее создать, а далее горизонтальное масштабирование, высокая доступность и все дополнительные работы, связанные с обеспечением работоспособности Internet gateway, она ложится на плечи AWS. Вам больше о нем переживать не нужно. Вторым шагом, который позволяет наш subnet сделать public, это настроить routing rules. Routing rules создаются в рамках Route tables. Route table - это то, что описывает как трафик может передвигаться внутри вашего VPC. В самом начале, когда вы создаете VPC, создается main route table стандартный. Рекомендуется создавать custom route table и уже его изменять. Более того, для каждого subnet рекомендуется создавать отдельный route table. Это позволит вам выдать только те rules, доступы, которые достаточно для конкретного subnet-а. Итак, мы вторым шагом создаем custom route table и в этом custom route table прописываем routing rule. В этом routing rule как destination, то есть назначение, мы указываем интернет. В нашем случае это будет IP адрес со всеми нулями слеш ноль. То есть это обозначение CIDR блока интернета. А как target указываем наш Internet gateway. Таким образом, все ресурсы, находящиеся в public subnet при обращении по сети к ресурсу из интернета, посмотрит в наш route table, увидит, что этот трафик направляется на Internet gateway и таким образом Internet gateway, приняв этот трафик, дальше пересылает ее в интернет и возвращает ответ обратно этому ресурсу. Следующий компонент, который хотелось бы вместе с вами разобрать, это Elastic IP address. Elastic IP address - это статический публичный IPv4 адрес, который может быть привязан к ресурсам в вашем VPC. Представим случай, у вас есть приложение, в public subnet-e находится Web tier, обрабатывает его EC2 инстанс с таким-то приватным IP адресом и к нему привязан Elastic IP адрес. По этому статическому IP адресу, ваш пользователь обращается к вашему приложению. Далее представим, что у вас возникли проблемы с Web tier и вы срочно создали другой EC2 instance и чтобы не менять конфигурации приложения, не менять код приложения, не просить пользователя обращаться к другому IP адресу, вы можете с легкостью этот IP адрес перепривязать к другому EC2 инстансу и прозрачно для всех, для приложения и пользователей они смогут успешно дальше продолжать работать, даже не заметив изменения EC2 инстанса, обрабатывающего их запросы. Следующий компонент Network - это NAT Gateway. Он используется в тех случаях, когда мы хотим предоставить доступ в интернет ресурсам из private subnet, так чтобы из интернета к ресурсам в этом private subnet не могли напрямую обратиться. Такое возможно и для того, чтобы это реализовать, нужно выполнить несколько шагов. Самым первым делом в public subnet у вас уже должен быть настроен выход в интернет. Для этого мы помним, первым шагом создается subnet, далее обязательно создается Internet Gateway, привязывается к VPC. После этого для public subnet создается отдельный кастомный routing table и в этом route table создается routing rule, который ассоциирован с этим subnet и в rule мы указываем, что как destination у нас указывается интернет, а как target указывается Internet Gateway. То есть мы выходим в интернет через Internet Gateway. После того, как мы все эти действия сделаем, у нас появляется доступ в интернет у ресурсов в public subnet и он фактически становится public, не только в названии. Теперь если мы говорим про вторую часть, это создание NAT Gateway, он создается в public subnet. Далее из private subnet у нас уже будет создан кастомный route table, в нем создается routing rule, который как destination укажет интернет, а как target укажет NAT Gateway. Как только NAT Gateway получит запрос от инстансов в private subnet, посмотрит на route table для своего subnet, увидит, что этот запрос должен быть направлен дальше в Internet Gateway и соответственно этот трафик направит дальше. Через Internet Gateway мы выйдем в интернет, ответ придет обратно к Internet Gateway, Internet Gateway направит его к NAT Gateway и NAT Gateway окончательный последний шаг вернет этот трафик, ответ на запрос инстансу, находящемся в private subnet. Давайте проведем небольшой квиз. Здесь нам необходимо для четырех видов нагрузок определить, где они должны находиться, в private или в public subnet. Вы можете остановить это видео и подумать некоторое время, как только будете готовы можете продолжить. Отлично, я думаю вы все ответили правильно, давайте проверим. Как Data store instances рекомендуется использовать private subnet. Это верно, так как из интернета доступ к нашим data store instances не нужно, они наоборот должны быть максимально защищены. В этом случае мы располагаем эти инстансы в private subnet. Далее, Batch-processing instances также относится к backend обработке и в этом случае из интернета к этим инстансам обращаться не нужно. В этом случае мы указываем private subnet. Для третьего случая уже вы по названию видите ответ Backend instances, они находятся на backend и также находятся в private subnet. Единственный случай - это Web application instances, они в зависимости от вашей архитектуры могут находиться либо в public subnet либо в private subnet. Я бы хотел обратить ваше внимание, что для большинства видов нагрузок и для самых сложных нагрузок мы используем private subnets, а лишь только для Web tier мы используем или даже в этом случае можем не использовать public subnets. В связи с этим при создании subnets, заранее зная какой он будет public либо private, вам рекомендуется создавать размеры public subnets меньше, а размеры private subnets больше. Таким образом вы с большей долей вероятности не исчерпаете диапазон доступных IP адресов для каждого из subnets. Следующий и пожалуй последний из основных компонентов VPC - это Bastion hosts. Bastion hosts используется для того, чтобы пользователи из офиса либо из домашнего компьютера могли подключаться к инфраструктуре находящейся в облаке. В этом случае в public subnet создается так называемый Bastion host это EC2 instance, для которого открыт доступ по 22 порту, то есть SSH трафик, на определенный range IP адресов. Если вы подключаетесь из локального офиса, то у вашего офиса скорее всего есть некоторый пул адресов, из которых вы выходите в интернет и вы можете для этого bastion host указать эти IP адреса, которые могут обращаться по 22 порту. Все другие порты должны быть закрыты. В этом случае из офиса можно будет подключиться к этому bastion host, далее с этого bastion host вы можете обратиться к любому из инстансов будь то в public subnet или в private subnet. Здесь следует обратить внимание, что bastion host является точкой входа из интернета, оно максимально подвержено атакам извне, поэтому необходимо использовать все возможные методы для обеспечения большей безопасности. Мы должны настроить security группы, network ACL и другие компоненты, которые у нас есть в наличии. Про безопасность мы с вами поговорим в последней части нашей сегодняшней лекции. В этой части мы подробно остановимся на таких компонентах VPC как security группы и network ACL. Это две основные компоненты, которые помогают нам настроить многоуровневую защиту в рамках нашего VPC. Начнем мы с Security groups. Security groups - это firewall, который привязывается на уровне инстанса, то есть вы указываете к какому инстансу его привязать. Важно понимать, что один инстанс может быть привязан только к одной security группе в один момент времени. А одна и та же security группа может быть использована различными инстансами. Security группы являются stateful firewall, это значит, что запоминается состояние. Если говорить другими словами, то тот трафик, который был разрешен inbound traffic, он вне зависимости от того какой outbound traffic разрешен, все равно отправит ответ. Обратное тоже верно. Если у нас разрешен outbound traffic, то вне зависимости от того какие inbound rules прописаны, мы все равно получим обратный ответ. Если же трафик иницируется со стороны инстанса, то здесь уже по-другому. Если у нас outbound traffic запрещен, но inbound traffic разрешен, то к этому инстансу могут обращаться только извне и получать ответ. Но сам инстанс так как нет разрешенного outbound трафика, то он не может никуда в наружу обращаться. Вы можете создавать кастомные security группы, есть дефолтовые security группы при создании subnets и по умолчанию весь inbound трафик запрещен, а весь outbound трафик разрешен. Если же вам нужно предоставить outbound трафик ограниченный, то вы можете удалить outbound rule в дефолтовой security группе или рекомендуется создать кастомную security группу и специально для этих EC2 инстансов с определенной одинаковой ролью создать security группы с соответствующими inbound и outbound rules. Давайте рассмотрим пример. У нас есть кастомная security группа она привязана к EC2 инстансу в public subnet-e, у этого инстанса есть приватный и публичный IP адрес. Так как у subnet прописан routing rule до Internet gateway, также Internet gateway привязан к нашему VPC, то этот EC2 инстанс может обращаться в интернет. Но давайте посмотрим, что же у нас прописано в кастомной security группе. Как вы видите outbound трафика нет, значит инстанс не может обращаться в интернет, но при этом прописан inbound rule это HTTP трафик по 80 порту и доступ предоставляется со всего интернета. Таким образом к этому инстансу из интернета по публичному IP адресу могут обратиться. Так как security группа это statefull firewall, то ответ успешно вернется, но при этом EC2 инстанс самостоятельно инициировать запрос в интернет не может, так как не прописан outbound трафик. Чтобы понять трафик разрешен или нет, оцениваются и проверяются все rules в security группе. Например, если к нам пришел какой-то inbound трафик, то все rules в рамках inbound трафика проверяются, в том случае, если она разрешается, то трафик проходит. Это же относится к outbound трафику. Есть также такое понятие как chaining security groups, идея в том, что для каждого типа EC2 инстанса в зависимости от того к какому tier она относится создавать отдельные security группы. Например, есть у нас Web tier security группа, которая содержит inbound rule, он разрешает доступ из интернета по 80 и 443 порту, это соответственно HTTP и HTTPS трафик. Далее, он разрешает доступ по SSH через 22 порт на диапазон IP адресов вашей корпоративной среды. Таким образом с локального офиса вы можете обращаться и подключаться к инстансам в Web tier. Далее, у нас есть отдельная security группа, которая предназначена для инстансов Application уровня. В этом случае, вы прописываете inbound rule также доступ по SSH, это 22 порт из диапазона корпоративных IP адресов. И более того прописываете inbound rule по 8000 порту HTTP трафик и при этом Source является наш Web tier. Исходя из этого мы видим, что к Application tier EC2 инстансам могут обратиться только трафик исходящий из Web tier. По такой же логике создается security группа для Database tier. Мы создаем inbound rule для подключения по SSH, а также создаем дополнительный inbound rule по порту 3306, это стандартный порт для базы данных MySQL и как source указываем Application tier. Здесь вы видите, что EC2 инстансы из Web tier соответствующей security группы не могут обратиться напрямую к Database tier, а могут обратиться только к Application tier. И, соответственно, все те EC2 инстансы с Application tier могут обратиться к Database уровню по определенному порту. Следующим компонентом, который помогает предоставить дополнительный уровень безопасности в рамках VPC это Network ACLs. Network ACLs расшифруется как Network Access Control Lists. Оно работает на уровне subnet-a. У каждого subnet-a может быть привязан только один Network ACL, тогда как один Network ACL может использоваться несколькими subnet-aми. Если же во время настройки VPC, subnet-ов, вы не указали настройки Network ACL, то к subnet привяжется дефолтовый Network ACL, который разрешает весь входящий и исходящий трафик. Следует различать, что Network ACL является stateless firewall, таким образом он не запоминает состояние, то есть откуда пришел запрос. Это говорит о том, что входящий трафик он должен быть обязательно разрешен в outbound rules для того, чтобы ваш ответ дошел до получателя. И обратный случай, когда вы что-то запрашиваете извне, помимо того что вы открываете outbound трафик, чтобы обратиться к сторонним ресурсам, вам необходимо также по тому же порту и протоколу открыть inbound трафик и только в этом случае вы сможете получать ответ. При необходимости вы можете создавать Custom Network ACLs в этом случае по умолчанию для Custom Network ACL все входящий и исходящий трафик запрещается. При необходимости вы можете добавить правила, которые разрешают по тому или другому порту. Здесь вы видите пример, что inbound трафик сотым приоритетом указано SSH подключение, которое разрешает из определенного CIDR блока. В нашем случае, мы видим CIDR блок размера 32, это говорит о том, что указан конкретный IP адрес. Далее, мы видим в конце приоритет звездочка, это тот rule, который отрабатывает в самом конце. Мы с вами говорили ранее, что Security группы проверяют все rules, если это inbound трафик, то проверяются все inbound rules, в случае outbound трафика проверяется все outbound rules. Когда мы говорим про Network ACL он проходит по всем соответствующим правилам в порядке приоритета и в тот момент, когда он видит правила, которые либо разрешают, либо запрещают, то оно продолжает с ним и остальные оставшиеся правила не рассматривает. Давайте рассмотрим типовой трафик, который проходит через несколько компонентов в рамках VPC. Представим, есть пользователи, они делают запрос по соответствующему IP адресу либо по доменному имени. Этот запрос приходит к Internet gateway, далее Internet gateway направляет трафик в route table. Route table определяет, можем ли мы этот трафик пустить или нет. Далее, если все хорошо, двигаемся дальше, до того как мы попадем в subnet мы добираемся до Network ACL. Проверяется все inbound rules, если все хорошо, то разрешается трафик и мы доходим до subnet. Внутри subnet у нас есть наши EC2 инстансы. До EC2 инстанса есть security группы, таким образом мы вначале как только попадаем в subnet проверяем соответствующую security группу. Если разрешается inbound трафик, то трафик пропускается и наш запрос доходит до EC2 инстанса. На уровне EC2 инстанса этот запрос обрабатывается и что происходит, когда оно дает ответ. В этом случае он также доходит до security группы. Мы помним, что security группа это stateful firewall. Значит в этом случае так как запрос был извне, outbound rules не проверяется. Если бы запрос исходил изначально от EC2 инстанса, то outbound rules проверились бы. А inbound rules, соответственно, пропустились бы. В этом случае outbound rules не проверяются. Дальше мы доходим до уровня subnet. От subnet мы доходим до Network ACL. На стороне Network ACL обязательно проверяются outbound rules, так как это stateless firewall. Если все хорошо мы доходим до route table и route table далее направляет трафик через Internet gateway до пользователей, которые отправили изначальный запрос. Вы видите, что у нас как минимум два уровня безопасности. Более того у нас есть route table, который может определять можно ли этот трафик пускать или нет. То есть несколько уровней безопасности в рамках нашего одного VPC, что является best practice и рекомендуется к использованию. На этом слайде небольшое повторение, чтобы закрепить как необходимо создать public subnet. Самым первым делом нам необходимо создать Internet gateway и привязать его к VPC. Далее происходит настройка route table. Мы должны как destination указать интернет. Обозначение интернета это IP адрес со всеми нулями слэш ноль и как target указывается Internet gateway. Далее нужно обратить внимание, что у EC2 инстанса есть публичный IP адрес либо elastic IP адрес, который не меняется. Публичный IP адрес, который выдаётся EC2 инстансом, динамический и в случае пересоздания инстанса может измениться. Также если мы останавливаем инстанс и после запускаем, то публичный IP адрес также может измениться и выдаться тот публичный IP адрес, который на момент запуска доступен. Но следует помнить, что при перезагрузке инстанса публичный IP адрес сохраняется. И последний шаг следует удостовериться, что на уровне EC2 инстанса security группы разрешают этот трафик, а на уровне subnet-a Network ACL также разрешает inbound и outbound трафик. Отлично, на этом мы добрались до конца четвертой части нашей сегодняшней лекции. Это заключительная часть, на этом мы завершаем всю сегодняшнюю лекцию. Подробнее разобрали сервис VPC, все компоненты, которые могут быть созданы в рамках этого VPC. Далее, мы познакомились с компонентами Security группы, Network ACL и другими основными компонентами, которые помогают нам построить безопасную и соответствующую всем best practice-ам локальную сеть в облаке. На этом слайде видите дополнительные источники, достаточно эти ключевые слова вбить в поисковик и в первой ссылке увидите необходимый материал. На этом мы завершаем нашу сегодняшнюю лекцию. Спасибо за внимание. Увидимся с вами на следующих наших активностях.