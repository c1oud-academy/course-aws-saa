 Добрый день, уважаемые студенты! Я рад вас всех видеть на очередной лекции. Мы с вами начинаем серию лекций, посвящённые нейтворкингу. На этой неделе мы поговорим про настройку сети в облаке AWS. На второй неделе у нас не будет лекций, но будет объёмная лабораторная работа. Она будет включать все те знания, которые вы получили на предыдущих лекциях, а также материалы, пройденные на последней лекции. На третьей неделе мы рассмотрим сервисы и варианты подключения локальной сети к сети в облаке AWS. Итак, давайте начнём. Сегодняшняя лекция состоит из четырёх частей. В первой части мы рассмотрим следующие улучшения нашей архитектуры в облаке, которые мы будем проделывать в рамках лабораторных работ. Во второй части подробнее познакомимся с сервисом VPC и её возможностями. В третьей части мы рассмотрим нюансы, связанные с VPC и её компонентами, которые дают нам возможность настроить подключение к интернету. И в последней части, четвёртой, мы рассмотрим нюансы, связанные с обеспечением безопасности нашего VPC. В первой части нашей лекции мы обсудим изменение инфраструктуры в облаке AWS в рамках лабораторных работ. Вы видите, что у нас появились новые компоненты, это VPC, интернет-гейтвей, NAT-гейтвей и три сабнета. Это всё работает согласованно и, во-первых, даёт большую безопасность, а во-вторых, у нас также все те ресурсы, к которым нужен выход в интернет, они также остались с этим доступом. Если говорить про бизнес-кейс, то работа в кафе продвигается хорошо, клиентов становится больше. София и Нихил освободились от своих основных работ и начали обсуждение по дальнейшему развитию IT-инфраструктуры в облаке AWS. Они посоветовались с постоянными посетителями кафе, которые работают с AWS, в нашем случае это Оливия, она является с Solution Architect и, выслушав текущее состояние инфраструктуры наших главных героев, она подсказала, что следует обратить особое внимание правильной настройки VPC, то есть сети и всех её компонентов, чтобы обезопасить инфраструктуру. Также она посоветовала для удаленного подключения и возможности проводить некоторые работы настроить Bastion Host. Что такое Bastion Host и другие компоненты мы с вами узнаем в нашей сегодняшней лекции. И мы сейчас переходим ко второй части нашей сегодняшней лекции. Во второй части нашей сегодняшней лекции мы подробнее ознакомимся с сервисом VPC и её компонентами. Что такое VPC? VPC расшифровывается как Virtual Private Cloud и оно нам предоставляет логически изолированную часть в облаке AWS. Оно максимально приближено к локальной сети, которую мы с вами настраивали много раз дома, на работе и AWS дает возможность удаленно настроить это в облаке AWS. У нас есть возможность выбирать CIDR блоки, то есть какой диапазон IP адресов мы хотим использовать для нашей локальной сети. Мы также можем создавать любое количество сабнетов в рамках размера нашего VPC. Внутри VPC мы можем настроить необходимые правила для того чтобы трафик мог перемещаться в таком или ином виде. Есть также все необходимые компоненты для обеспечения безопасности на уровне сабнета либо на уровне отдельно взятых ресурсов. Отличие при работе с AWS от локальных работ это то, что вам нет необходимости работать с железом. То есть вы не будете подключать кабель к сети, как-то их готовить и так далее. Все эти работы они уже проведены на стороне Tata центра AWS, уже все готово к использованию и вы удаленно, достаточно быстро можете настроить локальную сеть в облаке в зависимости от вашей бизнес задачи. VPC создается в рамках определенного AWS региона. То есть вы не можете создать VPC, который будет работать в нескольких AWS регионах. Если же вам нужно cross-region взаимодействия, то в каждом регионе создается отдельная VPC и эти VPC вы соединяете между собой. Как это делать мы с вами пройдем на следующих наших занятиях. Если говорить про availability зоны, то для VPC доступны все availability зоны в рамках региона, на котором оно было создано. Вы указываете все либо некоторые из availability зон, с которыми вы будете работать и соответственно ресурсы в рамках этого VPC могут быть созданы в той или иной availability зоне. Зачастую во время создания VPC указывают как минимум две availability зоны, что дает возможность построить highly available инфраструктуру в облаке AWS. Таким образом вы заранее обезопасите себя от возможных проблем в рамках availability зоны. В случае если что-то произойдет с одной availability зоной, другая часть вашей инфраструктуры продолжит работать во второй availability зоне. Во время создания VPC обязательно необходимо указать cidre блок. Cidre блок это диапазон IP адресов доступных для ресурсов создаваемых внутри VPC. Решифруется как classless inter domain routing. Cidre блок обозначается следующим форматом. Указывается начальный IP адрес и через слеш указывается размер cidre блока. Размер cidre блока может быть от 0 до 32. 0 это самый большой теоретически возможный cidre блок, а 32 это самый маленький, который состоит из одного IP адреса. Если вы посмотрите примеры, первая строчка это все нули и IP адрес, а также размерность cidre блока 0. Оно является фактически всем интернетом и затрагивает все возможные, теоретически возможные IPv4 адреса. Если мы говорим про cidre блок размера 32, то здесь у нас один единственный IP адрес и тот адрес, который начальный и является адресом внутри этого cidre блока. В нашем случае это 10.22.33.44. Давайте рассмотрим другой пример. Достаточно часто кейс это cidre блоки размера 24. Для этого cidre блока доступны 256 последовательных IPv4 адресов. Если мы говорим, что у нас начальный IP адрес 10.22.33.0, то есть вы видите последний компонент IP адреса 0, то самым последним является 255. То есть 10.22.33.255. Таким образом в этом cidre блоке ровно 256 IP адресов с учетом того, что IP адрес начинается с нуля. Для того, чтобы проще это обозначать, мы вместо последнего компонента четвертого указываем звездочку. Таким образом 10.22.33. говорит нам о том, что в этом cidre блоке 256 IP адресов. Давайте рассмотрим еще один популярный пример. Это cidre блок размера 16 и в этом случае оно содержит 65536 IP адресов. Оно считается следующим образом. Нам необходимо от 32 отнять размер нашего cidre блока и возвести в эту степень двойки. В нашем случае это будет 2 в степени 16. Оно в себе содержит 65000 адресов и для простоты обозначается следующим образом. Первые два компонента IP адреса она указывается и оставшиеся два, третий и четвертый указывается как звездочка. Таким образом мы понимаем, что каждый из третьего и четвертого компонента IP адреса может принимать значение от нуля до 255. Если мы говорим касательно AWS, то минимальный размер cidre блока в рамках AWS это 28. 28 это 16 IP адресов. Следует обратить внимание, что AWS резервирует для системного использования 5 IP адресов. Таким образом в cidre блоке 28 у вас теоретически есть 16 IP адресов, а в AWS вы отнимаете еще 5 адресов и для вас будут доступны 11 IP адресов. Самый большой максимально возможный cidre блок в рамках AWS это 16. Оно содержит в себе 65536 IP адресов. Это число запоминать не нужно. И если говорить фактическое количество IP адресов доступных для вас в AWS вы отнимаете также 5 IP адресов будет 65531. Вам нет необходимости запоминать какое количество IP адресов есть в том или ином cidre блоке. Достаточно знать формулу. Повторюсь мы это проходили в предыдущих лекциях. Идея в том, что вы берете размер пусть это будет 24 используйте следующую формулу 32-24 равняется 8 и вы возводите полученное значение. Возводите в эту степень двойку. Два в степени 8. Оно равняется 256. При необходимости вы зная формулу можете рассчитать какое количество IP адресов доступно в любом из cidre блоков. Я для вас рекомендую запомнить лишь некоторые самые популярные cidre блоки. Это как обозначается весь интернет все нули как обозначается один единственный IP адрес это слеш 32 и несколько cidre блоков это размера 28 что он состоит из 16 IP адресов. Потом cidre блок размера 24 что он состоит из 256 IP адресов и самый большой возможный это слеш 16 который состоит из 65 тысяч IP адресов. Мы с вами помним что VPC является некоторым изолированным периметром внутри облака AWS. Subnets же они входят в определенный VPC и их следует больше воспринимать как контейнеры или логические группы. Между этими группами вы можете настраивать routing policy. Routing policy мы с вами пройдем в следующей части нашей сегодняшней лекции. Когда вы создаете сабнеты вам также обязательно как и для VPC необходимо указать cidre блок. Cidre блоки сабнетов не должны пересекаться между собой. Это также верно для VPC. Cidre блоки ваших VPC не должны пересекаться между собой в рамках AWS аккаунта. VPC работает поверх нескольких availability zone, но в рамках одного региона. А сабнеты они работают в определенной availability zone и привязываются к нему во время создания. В рамках availability zone вы можете создать необходимое количество сабнетов. Здесь нет ограничений. Единственное ограничение это учитывать размеры вашего VPC, а также размеры ваших сабнетов. На примере справа вы видите, что у нас есть VPC следующим cidre блоком. Cidre блок размера 22 это 1024 IP адресов. Далее у нас есть 4 сабнета, 2 public сабнета, 2 private сабнета. По одному паблику и по одному private сабнету расположились в каждой availability zone и у каждого сабнета есть свой cidre блок размера 24. 24 он состоит из 256 IP адресов. А мы с вами помним, что AWS резервирует для себя 5 IP адресов, поэтому для нас доступны 251 IP адрес. Давайте остановимся на основных best practices, связанных с настройкой VPC. Рекомендуется в самом начале для VPC подключить как минимум две или более availability zone и строить ваше приложение таким образом, чтобы оно работало в нескольких availability zone. Таким образом сразу в начале ваше приложение будет высоко доступным. Если же вы начнете с одной availability zone, то в дальнейшем как только вы решите, что ваше приложение должно работать в нескольких availability zone, уже у вас будет какое-то готовое приложение, будут готовые нагрузки и это будет дольше по времени и трудозатратнее изменять вашу существующую архитектуру. Вследствием этого best practices является то, что рекомендуется размеры subnets в различных availability zone создавать одинаковых размеров. Это что-то логичное, но бывает такое, что создается в основной availability zone большие subnets большего размера, а в других поменьше. Если это будет проблемой в будущем, если же возникнут проблемы с availability zone, то у вас в других availability zone будут маленькие subnets и есть вероятность того, что не хватит IP адресов для обслуживания вашей нагрузки. Если же это будет равномерно, то вы не будете привязаны к одной конкретной availability zone и можете перекидывать основные нагрузки на любую из существующих availability zone. Далее, следующий best practice это то, что в рамках cidre блока VPC рекомендуется оставить некоторый запас. То есть, знайте, что VPC Cidre Блок и subnet Cidre Блок, он настраивается в самом начале, является обязательным параметром и далее он не может быть изменен. В случае subnet не так критично, потому что вы можете этот удалить и создать новый большего размера, а с VPC все намного сложнее, потому что вам в этом случае придется всю вашу инфраструктуру от одного VPC, который поменьше, переносить на VPC новую, которая побольше. Это прям большая проблема, поэтому в самом начале рекомендуется большой запас взять для размера VPC. Тем более мы за размеры VPC не оплачиваем, поэтому есть такая возможность брать большой запас. Если вы говорите, какой именно запас рекомендуется брать, я рекомендую отталкиваться от следующих показателей. Если вы только-только начинающий стартап, неизвестно какие нагрузки будут, вы можете попробовать теоретически рассчитать нагрузку на первые 100, либо 1000, либо 10 тысяч пользователей. Далее уже примерно расчет этих нагрузок даст понять, какой объем ресурсов вам нужен. И теперь представьте, что рассчитанный эталонный объем нагрузок увеличится в 1000 раз, и в этом случае что произойдет с вашей инфраструктурой, каких размеров она должна быть, чтобы обслужить эти нагрузки. Исходя из этого вы можете прикинуть оптимальный размер вашего VPC, чтобы в будущем не пришлось его пересоздавать. Если же у вас уже есть нагрузки либо в другом AWS аккаунте, может быть старый VPC, или вы с локальной инфраструктуры мигрируете в облако, то в этом случае у вас скорее всего уже есть определенные нагрузки, они стабильные, и ожидается, что по мере роста компании ваши нагрузки также будут расти. В этом случае нет необходимости переможать на 1000, это достаточно большой запас. В этом случае достаточно представить нагрузки в 100 раз больше, и в этом случае вы поймете, в каких местах какие ресурсы потребуют масштабирования. А какая-то часть ресурсов, она может остаться такой же и сильно масштабироваться не будет. Исходя из этого вы поймете, какой размер инфраструктуры будет, какое количество ресурсов и соответственно IP адресов вам будет нужно. И исходя из этого вы подберете оптимальный размер сидроблока вашего VPC. И последний best practice, он также является логичным, но не всегда учитывается компаниями. Часто бывает такое, что компания начинает мигрировать в облаке, у нее есть один AWS аккаунт, далее создается другой AWS аккаунт и другой департамент также начинает работать в другом AWS аккаунте со своим VPC, со своей инфраструктурой. Естественно в какой-то момент возникает вопрос, можно ли это все объединить. Так вот в случае, если у вас сидроблоки VPC из разных аккаунтов пересекаются, то вы никак не сможете между собой соединить, это практически будет нереально. Поэтому необходимо, чтобы IT архитектор в компании либо технический директор всегда учитывал эти моменты, проходило это все через него централизовано и диапазоны IP адресов, то есть сидроблоки для VPC, они выдавались так, чтобы не было пересечений. В этом случае в будущем, даже если у вас будут отдельные AWS аккаунты и потом вы решите все объединить, у вас не будет проблем со соединением этих VPC между собой. В случае же, если такая ситуация произошла, то единственное решение это ту VPC, которую проще перенести на другой VPC с сидроблоком не пересекающимся с существующими VPC, мигрировать. Это может потребовать достаточно много времени и трудозатрат от команды, поэтому у вас есть возможность, зная эти нюансы, в самом начале двигаться в правильном направлении и избежать возможных проблем в будущем. Давайте теперь подробнее поговорим про VPC диплоймент, то есть каким образом мы можем настроить VPC в зависимости от нашего приложения, размера компании, команды и так далее. Самый старый и традиционный подход это использовать один AWS аккаунт и все наши нагрузки, всю нашу инфраструктуру хостить в одном VPC. Это не рекомендуемый подход, но для маленьких команд, небольших команд и компании, оно может подойти. Более продвинутый вариант это multiple VPC, когда в одном AWS аккаунте у вас создаются отдельные VPC, таким образом вы ваши нагрузки изолируете друг от друга, но тем не менее эти нагрузки находятся в одном AWS аккаунте. Этот подход также подходит для небольших команд и дает возможность кратно масштабироваться в будущем, то есть никак не будет ограничивать команду. И самый продвинутый и рекомендуемый вариант это multiple accounts, когда у вас каждый environment вашего приложения диплоится в одном VPC в отдельном AWS аккаунте. Это хорошо подходит в принципе для любого размера компании, но особо рекомендуется для крупных организаций, компаний, где есть несколько команд, несколько департаментов и в этом случае каждая команда может двигаться независимо друг от друга. Тем не менее они все будут объединены между собой и могут управляться централизованно. На текущий момент есть очень много кастомных решений, инструментов, а также нативных сервисов, которые помогают управлять это большое количество AWS аккаунтов. Самый яркий пример это AWS Organizations и Control Tower. Это те сервисы, которые активно развиваются и очень сильно упрощают нам работу с несколькими AWS аккаунтами. Давайте приведу пример, что может находиться в разных AWS аккаунтах. Представьте у вас B2C приложение, ее используют пользователи, физические лица и в этом случае в каждом AWS аккаунте может находиться отдельный environment. Например представим в первом у нас Dev среда, во втором у нас среда тестирования, в третьем у нас Pre-Prot и в самом последнем у нас Prot среда, где вы обслуживаете боевые нагрузки. Другой пример, если вы B2B компания, то в этом случае у вас вашими партнерами являются другие компании. Для этих компаний вы можете создавать, выделять несколько AWS аккаунтов, в каждой из которых будет та или иная среда. Например у вас есть партнер компания A, крупный партнер, для него вы выделили 3 AWS аккаунта для каждого environment. Например для компании A для тестовой среды один аккаунт, для второго аккаунта мы загрузим среду разработки и в третьей у нас будет находиться Prot среда для этой компании. В этом случае нагрузки на среде разработки никак не отразятся на нагрузке в продакшенном среде. Таким образом вы максимально изолируете ваши нагрузки друг от друга и это может также быть полезным в рамках некоторых регуляторных требований, которые запрещают или рекомендуют нагрузки от разных компаний либо в зависимости от среды отделять внутри облака. Мы с вами знаем, что у каждого сервиса есть различные лимиты. Лимиты бывают soft и hard. Soft это те, которые могут быть увеличены, hard это те, которые не могут быть увеличены. Нужно всегда помнить о hard-лимитах, но есть яркий пример лимита в рамках Amazon VPC. Он часто приходит на экзамене, поэтому рекомендуется его запомнить. Для работы оно также вам поможет, но вы всегда можете открыть соответствующий сервис, посмотреть какие лимиты есть у любого из сервисов и дальше принимать решение во время вашей работы. Какой это лимит? Это то, что в рамках одного AWS аккаунта для каждого региона вы можете создавать не более 5 VPC. Этот лимит является soft, поэтому если вы сделаете запрос, то вам этот лимит могут увеличить. Отлично, мы с вами добрались до конца второй части нашей сегодняшней лекции и подробнее проговорили про сервис VPC, остановились на том, что такое CIDR блок, рассмотрели компонент VPC сабнеты и теперь двигаемся дальше к третьей части нашей сегодняшней лекции. Мы с вами начинаем третью часть нашей сегодняшней лекции и здесь посмотрим нюансы, связанные с подключением интернета к нашим ресурсам внутри VPC. Итак, мы говорили, что мы создаем VPC, внутри VPC создаются сабнеты и есть два вида сабнета public и private. Здесь очень важный момент, при создании сабнета мы нигде не ставим галочку, нет такого переключателя, который делает сабнет public или private. Мы лишь в названии сабнета указываем он public или private, подразумевая, что в случае указания public сабнета мы будем проводить дополнительные настройки, чтобы фактически сделать его public. Что такое public сабнет, это когда у ресурсов в этом сабнете есть выход в интернет и обратное тоже верно, из интернета до ресурсов в public сабнете тоже можно добраться, то есть обратиться напрямую. Когда мы говорим private сабнет, то там обратный случай, ресурсы находящиеся в private сабнете не могут напрямую выйти в интернет и также из интернета к этим ресурсам добраться также невозможно. Давайте теперь рассмотрим основные шаги, которые позволяют наш сабнет сделать фактически public. Для этого необходимо использовать компонент нетворка, называется интернет гейтвей. Интернет гейтвей создается как отдельный ресурс и привязывается к VPC. Создание интернет гейтвей это самый первый шаг. Этот сервис компонент является менеджед, поэтому вам достаточно ее создать, а далее горизонтальное масштабирование, высокая доступность и все дополнительные работы связанные с обеспечением работоспособности интернет гейтвея, она ложится на плечи AWS. Вам больше о нем переживать не нужно. Вторым шагом, который позволяет наш сабнет сделать паблик это настроить раутинг рулы. Раутинг рулы создаются в рамках road table. Road table это то, что описывает как трафик может передвигаться внутри вашего VPC. В самом начале, когда вы создаете VPC, создается main road table стандартный. Рекомендуется создавать кастомный road table и уже его изменять. Более того, для каждого сабнета рекомендуется создавать отдельный road table. Это позволит вам выдать только те рулы, доступы, которые достаточно для конкретного сабнета. Итак, мы вторым шагом создаем custom road table и в этом custom road table прописываем routing rule. В этом routing rule как destination, то есть назначение, мы указываем интернет. В нашем случае это будет IP адрес со всеми нулями слеш ноль. То есть это обозначение интернета. CIDR блок интернета. А как target указываем наш интернет gateway. Таким образом, все ресурсы, находящиеся в паблик сабнете при обращении по сети к ресурсу из интернета, посмотрит в наш road table, увидит, что этот трафик направляется на интернет gateway и таким образом интернет gateway, приняв этот трафик, дальше пересылает ее в интернет и возвращает ответ обратно этому ресурсу. Следующий компонент, который хотелось бы вместе с вами разобрать, это elastic IP адрес. Эластик IP адрес это статический публичный IPv4 адрес, который может быть привязан к ресурсам вашем VPC. Представим случай, у вас есть приложение, в паблик сабнете находится веб-тир, обрабатывает его EC2 instance с таким-то приватным IP адресом и к нему привязан эластик IP адрес. По этому статическому IP адресу, ваша пользователь обращается к вашему приложению. Далее представим, что у вас возникли проблемы с вашим веб-тир и вы срочно создали другой EC2 instance и для того, чтобы не менять конфигурации приложения, не менять код приложения, не просить пользователя обращаться к другому IP адресу, вы можете с легкостью этот IP адрес перепривязать к другому EC2 instance и прозрачно для всех, для приложения и для ваших пользователей они смогут успешно дальше продолжать работать, даже не заметив изменения EC2 instance, обрабатывающего их запросы. Следующий компонент Network это NAT Gateway. Он используется в тех случаях, когда мы хотим предоставить доступ в интернет ресурсам из private subnet, так чтобы из интернета к ресурсам в этом private subnet не могли напрямую обратиться. Такое возможно и для того, чтобы это реализовать, нужно выполнить несколько шагов. Самым первым делом в public subnet у вас уже должен быть настроен выход в интернет. Для этого мы помним, первым шагом создается subnet, далее обязательно создается Internet Gateway, привязывается к VPC. После этого для public subnet создается отдельный кастомный routing table и в этом routing table создается routing rule, который ассоциирован с этим subnet и в руле мы указываем, что как destination у нас указывается интернет, а как target указывается Internet Gateway. То есть мы выходим в интернет через Internet Gateway. После того, как мы все эти действия сделаем, у нас появляется доступ в интернет у ресурсов в public subnet и он фактически становится public, не только в названии. Теперь если мы говорим про вторую часть, это создание NAT Gateway, он создается в public subnet. Далее из private subnet у нас уже будет создан кастомный route table, в нем создается routing rule, который как destination укажет интернет, а как target укажет NAT Gateway. Как только NAT Gateway он получит запрос от инстенсов в private subnet, посмотрит на route table для своего subnet, увидит, что этот запрос должен быть направлен дальше в Internet Gateway и соответственно этот трафик направит дальше. Через Internet Gateway мы выйдем в интернет, ответ придет обратно к Internet Gateway, Internet Gateway направит его к NAT Gateway и NAT Gateway окончательный последний шаг вернет этот трафик, ответ на запрос инстанцу, находящемуся в private subnet. Давайте проведем небольшой квиз. Здесь нам необходимо для четырех видов нагрузок определить, где они должны находиться, в private или в public subnet. Вы можете остановить это видео и подумать некоторое время, как только будете готовы можете продолжить. Отлично, я думаю вы все ответили правильно, давайте проверим. Как data source, instances рекомендуется использовать private subnet? Это верно, так как из интернета доступ к нашим data store instances не нужно, они наоборот должны быть максимально защищены. В этом случае мы располагаем эти instances в private subnet. Далее batch processing instances это также относится к backend обработке и в этом случае из интернета к этим instance обращаться не нужно. В этом случае мы указываем private subnet. Для третьего случая уже вы по названию видите ответ backend instances, они находятся на backend и также находятся в private subnet. Единственный случай это web application instances, они в зависимости от вашей архитектуры могут находиться либо в public subnet либо в private subnet. Я бы хотел обратить ваше внимание, что для большинства видов нагрузок и для самых сложных нагрузок мы используем private subnets, а лишь только для вебтира мы используем или даже в этом случае можем не использовать public subnets. В связи с этим при создании subnets заранее зная какой он будет public либо private вам рекомендуется создавать размеры public subnets меньше, а размеры private subnets больше. Таким образом вы с большей долей вероятности не исчерпаете диапазон доступных IP адресов для каждого из subnets. Следующий и пожалуй последний из основных компонентов VPC это bastion host. Bastion host используется для того, чтобы пользователи из офиса либо из домашнего компьютера могли подключаться к инфраструктуре находящейся в облаке. В этом случае в public subnet создается так называемый bastion host это EC2 instance для которого открыть доступ по 22 порту то есть SSH трафик на определенный range IP адресов. Если вы подключаетесь из локального офиса то у вашего офиса скорее всего есть некоторые pool адресов из которых вы выходите в интернет и вы можете для этого bastion host указать эти IP адреса которые могут обращаться по 22 порту. Все другие порты они должны быть закрыты. В этом случае из офиса можно будет подключиться к этому bastion host далее с этого bastion host вы можете обратиться к любому из instance будь то в public subnet или в private subnet. Здесь следует обратить внимание что bastion host является точкой входа из интернета оно максимально подвержено атакам извне поэтому необходимо использовать все возможные методы для обеспечения большей безопасности. Мы должны настроить security группы, network ACL и все другие компоненты которые у нас есть в наличии. Про безопасность мы с вами поговорим в последней части нашей сегодняшней лекции. В этой части мы подробно остановимся на таких компонентах VPC как security группы и network ACL. Это две основные компоненты которые помогают нам настроить многоуровневую защиту в рамках нашего VPC. Начнем мы с security группы. Security группы это firewall который привязывается на уровне instance то есть вы указываете какому instance его привязать. Важно понимать что один instance может быть привязан только к одной security группы в один момент времени. А одна и та же security группа может быть использована различными instance. Security группы являются stateful firewall это значит что запоминается состояние. Если говорить другими словами то тот трафик который был разрушен inbound traffic он вне зависимости от того какой outbound traffic разрешен все равно отправит ответ. Обратное тоже верно. Если у нас разрешен outbound traffic то вне зависимости от того какие inbound рула прописаны мы все равно получим обратный ответ. Если же трафик иницируется со стороны instance то здесь уже по другому. Если у нас outbound traffic запрещен но inbound traffic разрешен то этому instance могут обращаться только извне и получать ответ. Но сам instance так как нет outbound разрешенных трафика то он не может никуда в наружу обращаться. Вы можете создавать кастомные security группы есть дефолтовые security группы при создании сабнетов и по умолчанию весь inbound трафик он запрещен а весь outbound трафик он разрешен. Если же вам нужно предоставить outbound трафик ограниченный то вы можете удалить outbound rule в дефолтовой security группе или рекомендуется создать кастомную security группу и специально для этих instance с определенной одинаковой роли создать security группы с соответствующими inbound и outbound рулами. Давайте рассмотрим пример. У нас есть кастомная security группа она привязана к EC2 instance в паблик сабнете у этого instance есть приватный и публичный IP адрес. Так как у subnet прописан routing rule до интернет гейтвея также интернет гейтвеи привязаны к нашему VPC то этот EC2 instance может обращаться в интернет. Но давайте посмотрим что же у нас прописано в кастомной security группе. Как вы видите outbound трафика нет значит instance не может обращаться в интернет но при этом прописан inbound rule единственный это HTTP трафик по 80 порту и доступ предоставляется со всего интернета. Таким образом к этому instance из интернета по публичному IP адресу могут обратиться. Так как security группа это стоит full firewall то ответ успешно вернется но при этом EC2 instance самостоятельно инициировать запрос в интернет не может так как не прописан outbound трафик. Для того чтобы понять трафик разрешен или нет оцениваются и проверяются все рулы в security группе. Например если к нам пришел пришел какой-то inbound трафик то все рулы в рамках inbound трафика они проверяются в том случае если она разрешается то трафик проходит. Это же относится к outbound трафику. Есть также такое понятие как chaining security groups идея в том что для каждого типа EC2 instance в зависимости от того к какому тиру она относится создавать отдельные security группы. Например есть у нас web tier security группа которая содержит inbound rule он разрешает доступ из интернета по 80 и 443 порту это соответственно HTTP и HTTPS трафик. Далее он разрешает доступ по SSH через 22 порт на диапазон IP адресов вашей корпоративной среды. Таким образом с локального офиса вы можете обращаться и подключаться к instance в web tier. Далее у нас есть отдельная security группа которая предназначена для instance application уровня. В этом случае вы прописываете inbound rule также доступ по SSH это 22 порт из диапазона корпоративных IP адресов. И более того прописываете inbound rule по 8000 порту HTTP трафик и при этом сурсом является наш web tier. Исходя из этого мы видим что к application tier EC2 instance могут обратиться только трафик исходящие из web tier. По такой же логике создается security группа для database tier. Мы создаем inbound rule для подключения по SSH а также создаем дополнительный inbound rule по порту 3306 это стандартный порт для базы данных mayscale и как source указываем application tier. Здесь вы видите что EC2 instance из web tier соответствующие security группы не могут обратиться напрямую к database tier а могут обратиться только к application tier. И соответственно все те EC2 instance с application tier могут обратиться к database уровню по определенному порту. Следующим компонентом который помогает предоставить дополнительный уровень безопасности в рамках VPC это Network ACLs. Network ACLs расшифруется как Network Access Control Lists. Оно работает на уровне сабнета. У каждого сабнета может быть привязано только один Network ACL тогда как один Network ACL может использоваться несколькими сабнетами. Если же во время настройки VPC сабнетов и не указали настройки Network ACL то к сабнету привяжется дефолтовый Network ACL который разрешает весь входящий и исходящий трафик. Следует различать что Network ACL является stateless firewall таким образом он не запоминает состояние то есть откуда пришел запрос. Это говорит о том что входящий трафик он должен быть обязательно разрушен в outbound рулах для того чтобы ваш ответ дошел до получателя. И обратный случай когда вы что-то запрашиваете извне помимо того что вы открываете outbound трафик чтобы обратиться к сторонним ресурсам вам необходимо также по тому же порту и протоколу открыть inbound трафик и только в этом случае вы сможете получать ответ. При необходимости вы можете создавать Custom Network ACLs в этом случае по умолчанию для Custom Network ACL все входящий и исходящий трафик она запрещается. При необходимости вы можете добавить правила которые разрушают по тому или другому порту. Здесь вы видите пример что inbound трафик сотым приоритетом указано SSH подключение которое разрешает из определенного CIDR блока. В нашем случае мы видим CIDR блок размера 32 это говорит о том что указан конкретный IP адрес. Далее мы видим в конце приоритет звездочка это тот рул который отрабатывает в самом конце. Мы с вами говорили ранее что Security группы они проверяют все рулы если это inbound трафика то проверяются все рулы inbound рулы в случае это outbound трафик проверяется все outbound рулы. Когда мы говорим про Network ACL он проходит по всем рулам соответствующим в порядке приоритета и в тот момент когда он видит правила которые либо разрешают либо запрещают то оно продолжает с ним и остальные оставшиеся правила не рассматривает. Давайте рассмотрим типовой трафик который проходит через несколько компонентов в рамках VPC. Представим у нас есть наши пользователи они делают запрос по соответствующему IP адресу либо по доменному имению. Этот запрос приходит к интернет гейтвею далее интернет гейтвей направляет трафик в road table. Road table определяет можем ли мы этот трафик пустить или нет. Далее если все хорошо мы двигаемся дальше до того как мы попадем в subnet мы добираемся до Network ACL. Проверяется все inbound рулы если все хорошо то разрешается трафик и мы доходим до subnet. Внутри subnet у нас есть наши institute инстанции. До institute инстанция есть security группы таким образом мы вначале как только попадаем в subnet проверяем соответствующую security группу. Если разрешается inbound трафик то трафик пропускается и наш запрос доходит до institute инстанция. На уровне institute инстанция этот запрос обрабатывается и что происходит когда оно дает ответ. В этом случае он также доходит до security группы. Мы с вами помним что security группа это stateful firewall. Значит в этом случае так как запрос был извне outbound рулы не проверяется. Если бы запрос исходил изначально от institute инстанция то outbound рулы проверились бы. А inbound рулы соответственно там пропустились бы. В этом случае outbound рулы не проверяются. Дальше мы доходим до уровня subnet. От subnet мы доходим до Network ACL. На стороне Network ACL обязательно проверяются outbound рулы так как это stateless firewall. Если все хорошо мы доходим до road table и road table далее направляет трафик через интернет gateway до пользователей которые отправили изначальный запрос. Вы видите что у нас как минимум два уровня безопасности. Более того у нас есть road table который может определять можно ли этот трафик пускать или нет. То есть несколько уровней безопасности в рамках нашего одного VPC что является best practice и рекомендуется к использованию. На этом слайде небольшое повторение чтобы закрепить как необходимо создать публичный subnet. Самым первым делом нам необходимо создать интернет gateway и привязать его к VPC. Далее происходит настройка road table. Мы должны как destination указать интернет. Обозначение интернета это IP адрес со всеми нулями slash ноль и как target указывается интернет gateway. Здесь далее обратить нужно внимание что у EC2 Instance есть публичный IP адрес либо elastic IP адрес который не меняется. Публичный IP адрес который выдаётся EC2 Instance он динамический и в случае пересоздания Instance может измениться. Также если мы останавливаем Instance и после запускаем то публичный IP адрес также может измениться и выдаться тот публичный IP адрес который на момент запуска доступен. Но следует помнить что при перезагрузке Instance публичный IP адрес сохраняется. И последний шаг следует удостовериться что на уровне EC2 Instance security группы разрешают этот трафик, а на уровне сабнета Network ACL также разрешает inbound и outbound трафик. Отлично на этом мы добрались до конца четвертой части нашей сегодняшней лекции. Это заключительная часть на этом мы совершаем всю сегодняшнюю лекцию. Подробнее разобрали сервис VPC все компоненты которые могут быть созданы в рамках этого VPC. Далее мы познакомились с компонентами Security группы, Network ACL и другими основными компонентами которые помогают нам построить безопасную и соответствующую всем best practice локальную сеть в облаке. На этом слайде вы видите дополнительные источники, вам достаточно идти ключевые слова вбить в поисковик и вы в первой ссылке увидите необходимый материал. На этом мы завершаем нашу сегодняшнюю лекцию. Спасибо за внимание. Увидимся с вами на следующих наших активностях.
